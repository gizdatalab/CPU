{"cells":[{"cell_type":"markdown","metadata":{"id":"gFYou2KuQAjD"},"source":["In this notebook we gather the NDC/LTS documents as provided by Climate Watch (html files) and also original pdf from UNFCC website.\n","\n","- Subsection (CW): Processes the html file to create the paragraphs based on split_by = 'words', split_length = [60,85,150] and split_overlap depends on country.\n","- Subsection (IKI): Document downlaod using links info, if fails tries to find document in other date sub-domains of website. Extract text and then create paragraphs."]},{"cell_type":"markdown","metadata":{"id":"8wROI3cZWvjf"},"source":["# Packages Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LRD9LHCvyfl"},"outputs":[],"source":["# # %%capture\n","# # linux packages\n","!wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.04.tar.gz\n","!tar -xvf xpdf-tools-linux-4.04.tar.gz && sudo cp xpdf-tools-linux-4.04/bin64/pdftotext /usr/local/bin\n","!apt-get install tesseract-ocr libtesseract-dev poppler-utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ma_2N73MXIlY"},"outputs":[],"source":["!pip3 install pycountry\n","!pip install rank_bm25\n","!pip install xlsxwriter\n","!pip install -e \"git+https://github.com/gizdatalab/haystack_utils.git@main#egg=utils\"\n","!pip install langdetect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6jcBPyrXsq5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from typing import Callable, Dict, List, Optional, Text, Tuple, Union\n","import pycountry\n","from bs4 import BeautifulSoup\n","from IPython.display import display\n","import os\n","import glob\n","import utils\n","from utils.preprocessing import UdfPreProcessor\n","from haystack.schema import Document\n","from tqdm import tqdm\n","import requests\n","import json\n","import hashlib\n","import os\n","import re\n","import urllib.parse\n","from pathlib import Path\n","import ast\n","import requests"]},{"cell_type":"markdown","metadata":{"id":"UA30YNQQYAM-"},"source":["# Processing Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1702968560160,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"L46vh7EOYRjE","outputId":"f3d233be-cc86-4027-fc4d-cd949ba5d96e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-8dfee116-eaf1-4057-9989-9f2f5bb37df7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Alpha3</th>\n","      <th>Country</th>\n","      <th>value_count</th>\n","      <th>mean_val</th>\n","      <th>median_val</th>\n","      <th>min_val</th>\n","      <th>max_val</th>\n","      <th>std_val</th>\n","      <th>ninetyfifth_percentile</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>217</th>\n","      <td>Global</td>\n","      <td>Global</td>\n","      <td>NaN</td>\n","      <td>17.069639</td>\n","      <td>10.0</td>\n","      <td>NaN</td>\n","      <td>85.7875</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dfee116-eaf1-4057-9989-9f2f5bb37df7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8dfee116-eaf1-4057-9989-9f2f5bb37df7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8dfee116-eaf1-4057-9989-9f2f5bb37df7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["     Alpha3 Country  value_count   mean_val  median_val  min_val  max_val  \\\n","217  Global  Global          NaN  17.069639        10.0      NaN  85.7875   \n","\n","     std_val  ninetyfifth_percentile  \n","217      NaN                     NaN  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," Global Median value of Response Length: 10.0\n"]}],"source":["# setting up the paths to climate watch data\n","path_to_step1 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step1/\"\n","\n","# reading the file which contains the response statistics at country level\n","responsestats = pd.read_json(path_to_step1 + 'output/responsestats.json')\n","\n","display(responsestats[responsestats.Country == 'Global'])\n","responseLengthMedian = (responsestats[responsestats.Country == 'Global']['median_val'].values)[0]\n","print('\\n',\"Global Median value of Response Length:\", responseLengthMedian)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0a1E4TcYBCt"},"outputs":[],"source":["def get_split_strategy(country_median):\n","    \"\"\"\n","    builds 3 split_strategy based on Responsetext length stats of country. While\n","    it is good to have a overlap, however too much of overlap can lead to duplications.\n","    So we try to restrict the overlap to either 10 words, if the country\n","    median < 20 or if its more than 20, we limit overlap to 20 words.\n","\n","\n","    Params\n","    -----------------\n","    country_median\n","\n","\n","    Return\n","    -----------------\n","    split_strategy:List[Dict], Dictionary contains split length, overlap, split_by etc\n","\n","    \"\"\"\n","    #\n","    if country_median < 20:\n","        split_strategy = [{'split_length':60, \"split_by\": 'word',\n","                            'split_overlap':int(responseLengthMedian),\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':85, \"split_by\": 'word',\n","                            'split_overlap':int(responseLengthMedian),\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':150, \"split_by\": 'word',\n","                            'split_overlap':int(responseLengthMedian),\n","                            \"split_respect_sentence_boundary\": True}]\n","\n","    else:\n","        split_strategy = [{'split_length':60,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':85,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':150,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True}]\n","\n","    return split_strategy\n","\n","def get_country_split_strategy(country_:None,country_code):\n","    \"\"\"\n","    try to get the country response stats and then calls get_split_strategy to\n","    fetch splitting strategies, in case of some error returns defined staretgies\n","\n","    \"\"\"\n","    try:\n","        country_mean  = country_[country_.Alpha3 == country_code].mean_val.values[0]\n","        country_max = country_[country_.Alpha3 == country_code].max_val.values[0]\n","        country_median = country_[country_.Alpha3 == country_code].median_val.values[0]\n","\n","        split_strategy = get_split_strategy(country_mean, country_max, country_median)\n","\n","        return split_strategy\n","\n","    except:\n","        return [{'split_length':60,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':85,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True},\n","                          {'split_length':150,\"split_by\": 'word',\n","                            'split_overlap':20,\n","                            \"split_respect_sentence_boundary\": True}]\n","\n","def create_paragraphs_text(text_data, split_strategy, language='en', filename= None):\n","    \"\"\"\n","    Takes Raw text data extracted from html files, pdf files etc and splits this\n","    text into small chunks based on split strategy.\n","\n","    Params\n","    ----------------\n","    text_data: raw text data\n","    split_strategy: List[Dict], Dictionary contains split length, overlap, split_by etc\n","                    as requried for paragraph/text_chunks creation from raw text.\n","\n","    language: language of text. The Preprocessor component allows to pass the\n","              langauge so that in the backend we can use punkt tokenizer of that\n","              language to create the paragraphs\n","    filename: Optional, filename to which the text data belongs to.\n","\n","    Return\n","    ---------------\n","    placeholder:\n","\n","    \"\"\"\n","\n","    # placeholder to collect the paragraphs as per strategy\n","    placeholder = {}\n","    # creating the haystack document from text_data\n","    documents = Document(content=text_data,\n","                          meta={\"name\": filename},\n","                          id_hash_keys=None)\n","\n","    # starting preprocessing for paragraph creation\n","    custom_preprocessor = UdfPreProcessor()\n","    if split_strategy:\n","        for strategy in split_strategy:\n","            output = custom_preprocessor.run([documents],\n","                                split_by = \"word\",\n","                                remove_punc = False,\n","                                split_respect_sentence_boundary = True,\n","                                split_length = strategy['split_length'],\n","                                split_overlap = strategy['split_overlap'],\n","                                language = language)\n","            passages = output[0]['documents']\n","            new_para_list = []\n","            for passage in passages:\n","                new_para_list.append((passage.content,passage.meta['name'],\n","                                      passage.meta['page'], passage.id))\n","            if new_para_list:\n","                placeholder[str(strategy)] = new_para_list\n","    return placeholder\n","\n","\n","\n","def paraLengthCheck(paraList, max_len = 180, keep_hash = True):\n","    \"\"\"\n","    There are cases where preprocessor cannot respect word limit, when using\n","    respect sentence boundary flag due to missing sentence boundaries.\n","    Therefore we run one more round of split here for those paragraphs\n","\n","    Params\n","    ---------------\n","    paraList : list of paragraphs/text\n","    max_len : max length to be respected by sentences which bypassed\n","              preprocessor strategy\n","\n","    Return\n","    ----------\n","    paragraph_list:\n","\n","    \"\"\"\n","\n","    splits = list(paraList.keys())\n","    paragraph_list = {}\n","\n","    for split in splits:\n","        new_para_list = []\n","        paragraphs = paraList[split]\n","        for passage in paragraphs:\n","            # check if para exceeds words limit\n","            if len(passage[0].split()) > max_len:\n","              # we might need few iterations example if para = 512 tokens\n","              # we need to iterate 5 times to reduce para to size limit of '100'\n","                iterations = int(len(passage[0].split())/max_len)\n","                for i in range(iterations):\n","                    if i == 0:\n","                        temp  = \" \".join(passage[0].split()[max_len*i:max_len*(i+1)])\n","                    else:\n","                        # overlap 20 tokens from previous paragraph\n","                        temp = \" \".join(passage[0].split()[(max_len*i) - 20:max_len*(i+1)])\n","                    new_para_list.append([temp,passage[1],passage[2]])\n","                temp  = \" \".join(passage[0].split()[max_len*(i+1)-20:])\n","\n","                # Passages\n","                if keep_hash:\n","                    new_para_list.append([temp,passage[1],passage[2],passage[3]])\n","                else:\n","                    new_para_list.append([temp,passage[1],passage[2]])\n","            else:\n","                # paragraphs which dont need any splitting\n","                if keep_hash:\n","                    new_para_list.append([passage[0],passage[1],passage[2],passage[3]])\n","                else:\n","                    new_para_list.append([passage[0],passage[1],passage[2]])\n","        paragraph_list[split] = new_para_list\n","        # logging.info(\"New paragraphs length {}\".format(len(new_para_list)))\n","    return paragraph_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhdhQEh9kqdy"},"outputs":[],"source":["\n","# https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python\n","def htmlparser(filepath):\n","    \"\"\"\n","    Reads the html file and returns the text/string\n","\n","    Params\n","    ---------\n","    filepath: path to the html file\n","\n","    Return\n","    ------------\n","    full_text: partially cleaned text from html file\n","\n","    \"\"\"\n","    filetest = open(filepath, \"r\")\n","    soup = BeautifulSoup(filetest, features=\"html.parser\")\n","\n","    # kill all script and style elements\n","    for script in soup([\"script\", \"style\"]):\n","        script.extract()    # rip it out\n","    # get text\n","    text = soup.get_text()\n","\n","    # avoiding the text with semicolon to be split across lines\n","    text = text.replace(\":\\n\",\":\")\n","    list_lines = [line.strip() for line in text.splitlines()]\n","\n","    # break multi-headlines into a line each\n","    list_chunks = [phrase.strip() for line in list_lines for phrase in line.split(\"  \")]\n","\n","    new_list = [chunk for chunk in list_chunks if chunk]\n","\n","\n","    # print(\"Total lines without blanks:\",len(new_list))\n","\n","    # creating text from list of lines\n","    # haystack split for sentences doesnt work if there is no space\n","    # between sentences therefore adding full-stop with space to join lines.\n","    full_text = '. '.join(new_list)\n","\n","    # removing \"..\" as previous startegy might cause this at some places.\n","    full_text = full_text.replace(\"..\",\".\")\n","    return full_text\n","\n","\n","def extract_cwinfo(filename_):\n","    \"\"\"\n","    this is to extract some relvant information from CW provided NDC html filenames\n","    Usual structure of filename is like \"AND-revised_first_ndc-EN_TR.html\"\n","    So if we split by \"-\", we have Country Code, Filename, and Langauge.\n","\n","    Params\n","    ---------------\n","    filename_: filename of html files from CW github repo\n","\n","\n","    Return\n","    ------------------\n","    country_code: ISO Alpha3 code of country\n","    doc_type: Name of the document type, given as its mostly NDC documents it\n","              would be like 'First NDC', 'Revised First NDC' etc....\n","    lang: Langauge code for the document like 'EN', 'ES' etc\n","\n","    \"\"\"\n","\n","\n","    tokens = filename_.split(\"-\")\n","    if len(tokens) == 4:\n","        country_code = tokens[0]\n","        doc_type =  \" \".join(tokens[1].upper().split(\"_\"))\n","        lang = tokens[-2]\n","    else:\n","        country_code = tokens[0]\n","        doc_type =  \" \".join(tokens[1].upper().split(\"_\"))\n","        lang = (tokens[-1].split(\".\")[0]).split(\"_\")[0]\n","\n","    return country_code, doc_type, lang.strip().lower()"]},{"cell_type":"markdown","metadata":{"id":"CwZjQH_dQQXJ"},"source":["# CW"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7630,"status":"ok","timestamp":1702912427465,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"O_CUXDRVPuvg","outputId":"e9fb48ed-d7ca-4380-d209-d74de29b2883"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into '/content/policyData/ndc'...\n","remote: Enumerating objects: 4819, done.\u001b[K\n","remote: Counting objects: 100% (759/759), done.\u001b[K\n","remote: Compressing objects: 100% (702/702), done.\u001b[K\n","^C\n"]}],"source":["# getting ndc html files from climate watch git repo\n","!git clone https://github.com/WRI-ClimateWatch/ndc /content/drive/MyDrive/Colab\\ Notebooks/CPU/Step2/input/cw_ndc/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukYj8onwgmPm"},"outputs":[],"source":["# path where we cloned the climate watch ndc repo\n","path_to_ndc = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/cw_ndc/\"\n","\n","# read all html files\n","cw_ndc_files = glob.glob(path_to_ndc+'*.html')\n","\n","# create dataframe and extract info from filename\n","cw_ndc = pd.DataFrame(cw_ndc_files, columns=['filepath'])\n","\n","# extarct filename\n","cw_ndc['file_name'] = cw_ndc['filepath'].apply(lambda x: os.path.basename(x))\n","\n","# extract information from filename\n","cw_ndc['country_code'], cw_ndc['type_of_document'], cw_ndc['language'] = \\\n","                  zip(*cw_ndc.file_name.apply(lambda x: extract_cwinfo(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1702973243604,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"3QI0MvyQiz6B","outputId":"711d7140-66db-4e28-d359-69f922611d2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                            filepath  \\\n","158  /content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/cw_ndc/CRI_revised_fi...   \n","159  /content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/cw_ndc/CRI_revised_fi...   \n","\n","                            file_name           country_code type_of_document  \\\n","158  CRI_revised_first_ndc-EN_TR.html  CRI_revised_first_ndc       EN TR.HTML   \n","159     CRI_revised_first_ndc-ES.html  CRI_revised_first_ndc          ES.HTML   \n","\n","    language  \n","158       en  \n","159       es  "],"text/html":["\n","  <div id=\"df-2c59e729-32dd-46f1-9d6d-bc242adabad5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filepath</th>\n","      <th>file_name</th>\n","      <th>country_code</th>\n","      <th>type_of_document</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>158</th>\n","      <td>/content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/cw_ndc/CRI_revised_fi...</td>\n","      <td>CRI_revised_first_ndc-EN_TR.html</td>\n","      <td>CRI_revised_first_ndc</td>\n","      <td>EN TR.HTML</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>159</th>\n","      <td>/content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/cw_ndc/CRI_revised_fi...</td>\n","      <td>CRI_revised_first_ndc-ES.html</td>\n","      <td>CRI_revised_first_ndc</td>\n","      <td>ES.HTML</td>\n","      <td>es</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c59e729-32dd-46f1-9d6d-bc242adabad5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2c59e729-32dd-46f1-9d6d-bc242adabad5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2c59e729-32dd-46f1-9d6d-bc242adabad5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-aee83b2d-a661-48b5-8d57-aa5127c3cf82\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aee83b2d-a661-48b5-8d57-aa5127c3cf82')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-aee83b2d-a661-48b5-8d57-aa5127c3cf82 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":55}],"source":["# Some filenames can not follow the generic structure and we need to edit those entries\n","cw_ndc[cw_ndc.type_of_document.str.contains('HTML')]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1702973273337,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"GpsxLr7Mje_Q","outputId":"e991a5c6-1c48-4df2-d9fa-f2f32f5dbc6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["['FIRST NDC' 'INDC' 'REVISED FIRST NDC' 'SECOND NDC' 'ARCHIVED SECOND NDC'\n"," 'ARCHIVED REVISED FIRST NDC' 'REVISED SECOND NDC']\n"]}],"source":["# few manual corrections\n","cw_ndc.loc[158,'type_of_document'] = 'REVISED FIRST NDC'\n","cw_ndc.loc[159,'type_of_document'] = 'REVISED FIRST NDC'\n","cw_ndc.loc[158,'country_code'] = 'CRI'\n","cw_ndc.loc[159,'country_code'] = 'CRI'\n","\n","print(cw_ndc.type_of_document.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49869,"status":"ok","timestamp":1702973330554,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"LyRLfhPrkCx-","outputId":"0641da70-f041-444a-8bfe-57a6a81a5272"},"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting raw text from html files\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 634/634 [00:49<00:00, 12.80it/s]\n"]}],"source":["tqdm.pandas()\n","# read html file and get full cleaned text\n","print(\"Extracting raw text from html files\")\n","cw_ndc['original_text'] = cw_ndc['filepath'].progress_apply(lambda x: htmlparser(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mk7GO-gYmYsZ"},"outputs":[],"source":["# create paragraphs from extracted raw text\n","print(\"Creating Paragraphs\")\n","cw_ndc['paragraphs'] = cw_ndc.progress_apply(lambda x: create_paragraphs_text(x['original_text'],\n","                                    get_country_split_strategy(responsestats,x['country_code']),\n","                                    x['language'], x['file_name']), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iije2w83gBwv"},"outputs":[],"source":["# do the paragraph check for this we use the default value of 180 words for each\n","# split type. We do this so that the while training of Models, we dont get error\n","# due to tensor size mismatch. alternatively you cna also set the truncation =True\n","# for model tokenizers.\n","cw_ndc['paragraphs'] = cw_ndc.paragraphs.apply(lambda x: paraLengthCheck(x, keep_hash=False))\n","\n","# Filetype renaming\n","cw_ndc.type_of_document.replace({'FIRST NDC': 'First NDC',\n","                                'REVISED FIRST NDC': 'Revised First NDC',\n","                                'ARCHIVED REVISED FIRST NDC': 'Revised First NDC (archived)',\n","                                'SECOND NDC':'Second NDC',\n","                                'REVISED SECOND NDC':'Revised Second NDC',\n","                                'ARCHIVED SECOND NDC':'Archived Second NDC'},\n","                                inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2762,"status":"ok","timestamp":1702974206773,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"},"user_tz":-60},"id":"V8p8pRJFoQQl","outputId":"f07603b9-e778-4e63-c791-3c76447d1eef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Document types: ['First NDC' 'INDC' 'Revised First NDC' 'Second NDC' 'Archived Second NDC'\n"," 'Revised First NDC (archived)' 'Revised Second NDC'] \n","\n","country codes: ['AFG' 'AGO' 'ALB' 'AND' 'ARE' 'ARG' 'ARM' 'ATG' 'AUS' 'AUT' 'AZE' 'BDI'\n"," 'BEL' 'BEN' 'BFA' 'BGD' 'BGR' 'BHR' 'BHS' 'BIH' 'BLR' 'BLZ' 'BOL' 'BRA'\n"," 'BRB' 'BRN' 'BTN' 'BWA' 'CAF' 'CAN' 'CHE' 'CHL' 'CHN' 'CIV' 'CMR' 'COD'\n"," 'COG' 'COK' 'COL' 'COM' 'CPV' 'CRI' 'CUB' 'CYP' 'CZE' 'DEU' 'DJI' 'DMA'\n"," 'DNK' 'DOM' 'DZA' 'ECU' 'EGY' 'ERI' 'ESP' 'EST' 'ETH' 'EUU' 'FIN' 'FJI'\n"," 'FRA' 'FSM' 'GAB' 'GBR' 'GEO' 'GHA' 'GIN' 'GMB' 'GNB' 'GNQ' 'GRC' 'GRD'\n"," 'GTM' 'GUY' 'HND' 'HRV' 'HTI' 'HUN' 'IDN' 'IND' 'IRL' 'IRN' 'IRQ' 'ISL'\n"," 'ISR' 'ITA' 'JAM' 'JOR' 'JPN' 'KAZ' 'KEN' 'KGZ' 'KHM' 'KIR' 'KNA' 'KOR'\n"," 'KWT' 'LAO' 'LBN' 'LBR' 'LCA' 'LIE' 'LKA' 'LSO' 'LTU' 'LUX' 'LVA' 'MAR'\n"," 'MCO' 'MDA' 'MDG' 'MDV' 'MEX' 'MHL' 'MKD' 'MLI' 'MLT' 'MMR' 'MNE' 'MNG'\n"," 'MOZ' 'MRT' 'MUS' 'MWI' 'MYS' 'NAM' 'NER' 'NGA' 'NIC' 'NIU' 'NLD' 'NOR'\n"," 'NPL' 'NRU' 'NZL' 'OMN' 'PAK' 'PAN' 'PER' 'PHL' 'PLW' 'PNG' 'POL' 'PRK'\n"," 'PRT' 'PRY' 'PSE' 'QAT' 'ROU' 'RUS' 'RWA' 'SAU' 'SDN' 'SEN' 'SGP' 'SLB'\n"," 'SLE' 'SLV' 'SMR' 'SOM' 'SRB' 'SSD' 'STP' 'SUR' 'SVK' 'SVN' 'SWE' 'SWZ'\n"," 'SYC' 'SYR' 'TCD' 'TGO' 'THA' 'TJK' 'TKM' 'TLS' 'TON' 'TTO' 'TUN' 'TUR'\n"," 'TUV' 'TZA' 'UGA' 'UKR' 'URY' 'USA' 'UZB' 'VCT' 'VEN' 'VNM' 'VUT' 'WSM'\n"," 'YEM' 'ZAF' 'ZMB' 'ZWE'] \n","\n","languages types: ['en' 'fr' 'es' 'ru' 'zh' 'ar'] \n","\n"]}],"source":["# data validations\n","print('Document types:',cw_ndc.type_of_document.unique(),'\\n')\n","print('country codes:',cw_ndc.country_code.unique(),'\\n')\n","print('languages types:',cw_ndc.language.unique(),'\\n')"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"-WXQSC0GovzZ","executionInfo":{"status":"ok","timestamp":1702974546734,"user_tz":-60,"elapsed":7108,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}}},"outputs":[],"source":["path_to_step2 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/\"\n","jsonfile = cw_ndc.to_json(orient=\"records\")\n","parsed = json.loads(jsonfile)\n","with open(path_to_step2 +'output/html_para.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)"]},{"cell_type":"code","source":["cw_ndc.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8mbFQHGNKBz","executionInfo":{"status":"ok","timestamp":1702974765175,"user_tz":-60,"elapsed":3,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"cccde5b0-338a-4e4e-bbb4-e5299c7a8db3"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 634 entries, 0 to 633\n","Data columns (total 7 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   filepath          634 non-null    object\n"," 1   file_name         634 non-null    object\n"," 2   country_code      634 non-null    object\n"," 3   type_of_document  634 non-null    object\n"," 4   language          634 non-null    object\n"," 5   original_text     634 non-null    object\n"," 6   paragraphs        634 non-null    object\n","dtypes: object(7)\n","memory usage: 34.8+ KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"o66h5JFTbl-Y"},"source":["# IKI"]},{"cell_type":"markdown","metadata":{"id":"kbmNfcxTbz8V"},"source":["## Doc Collection\n","\n","This subsection deals with fetching the documents from url links, alternatively explore website in case relevant document is not found. You can add some documents link (and place files in relevant folder) manually too and update the files list before processing next section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7JJEBa9cuFJ"},"outputs":[],"source":["path_to_step1 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step1/\"\n","df_documents = pd.read_csv(path_to_step1 + \"output/documents_list.csv\")\n","\n","print(f\"Number of urls: {len(df_documents)}\")\n","df_documents = df_documents.dropna(how='any', subset=['url', 'country_code',\n","                                                      'type_of_document'])\n","df_documents = df_documents.astype(str)\n","df_documents = df_documents.reset_index(drop = True)\n","df_documents['valid'] = False\n","\n","print(f\"Number of  valid url after cleaning: {len(df_documents)}\")\n","\n","# Check accessible documents\n","# Using a user agent header to not get banned by the CDN of UNFCCC\n","# response can vary based on User agent so be cautious\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n","}\n","\n","\n","def valid_doc(url):\n","    try:\n","        res = requests.head(url, headers=headers)\n","    except Exception as e:\n","        return False\n","    if res.status_code == 200:\n","        return True\n","    else:\n","        return False\n","\n","\n","# select valid documents\n","for i in tqdm(range(len(df_documents))):\n","    df_documents.loc[i,'valid' ] = valid_doc(df_documents.loc[i,'url'])\n","\n","print('Number of valid documents: ', len(df_documents[df_documents['valid'] == True]))\n","print('Number of invalid documents: ', len(df_documents[df_documents['valid'] == False]))\n"]},{"cell_type":"markdown","metadata":{"id":"3IwHhDT0hQNr"},"source":["We try to find some more valid by looping through a series of date subdomains (YYYY-MM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"em1zlA5PeS9k"},"outputs":[],"source":["# Review missing URLs\n","# Using a user agent header to not get banned by the CDN of UNFCCC\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n","}\n","\n","# Get the last part of the url and loop through a series of date subdomains (YYYY-MM),\n","# testing for valid documents. If valid, get updated url.\n","def valid_doc_find(url, date_dir):\n","    for date in range(0, len(date_dir)):\n","        try:\n","            url_new = 'https://unfccc.int/sites/default/files/NDC/' + date_dir[date] \\\n","                                                      + '/' + url.split('/')[-1]\n","            res = requests.head(url_new, headers=headers)\n","        except:\n","            res = None\n","        if res:\n","            if res.status_code == 200:\n","              return url_new\n","    return None\n","\n","\n","# Set dates vector\n","dates = ['2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12','2023-01','2023-02','2023-03','2023-04','2023-05']\n","\n","# valid_doc_find output to new column\n","df_documents['url_found_prg'] = None\n","for i in tqdm(range(len(df_documents))):\n","    if df_documents.loc[i,'valid'] == False:\n","        df_documents.loc[i,'url_found_prg'] = valid_doc_find(df_documents.loc[i,'url'], dates)\n","\n","\n","# update the url\n","df_documents['url'] = df_documents.apply(lambda x: x['url'] if x['valid'] == True\n","                          else ( x['url_found_prg'] if x['url_found_prg'] != None\n","                                else None), axis=1)\n","# making final validity check\n","df_documents['valid_final'] = df_documents.url.apply(lambda x: True if x else False)\n","print('Number of valid documents: ', len(df_documents[df_documents['valid_final'] == True]))\n","print('Number of invalid documents: ', len(df_documents[df_documents['valid_final'] == False]))\n","# drop the invalidated rows\n","df_documents = df_documents[df_documents['valid_final'] == True].drop(columns=['url_found_prg',\n","                                                                  'Unnamed: 0'])\n","df_documents = df_documents.reset_index(drop = True)\n","df_documents['file_type'] = df_documents.url.apply(lambda x: '.docx' if x.split('.')[-1] == 'docx' else '.pdf')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4WLewAKkMXp"},"outputs":[],"source":["# download documents\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n","}\n","\n","def download_doc(url):\n","    filename = hashlib.sha256(urllib.parse.unquote(url).encode('utf-8')).hexdigest()\n","    filename = Path(os.path.join('/content', 'data', 'downloaded_documents', filename + '.pdf'))\n","\n","    if filename.exists():\n","        return filename.__str__()\n","    try:\n","        res = requests.get(url, headers=headers)\n","    except:\n","        print('Error downloading: ', url)\n","        return ''\n","\n","    if not res.ok:\n","        print('Error downloading: ', url)\n","        return ''\n","\n","    try:\n","        with open(filename, \"wb\") as fd:\n","            for chunk in res.iter_content(chunk_size=128):\n","                fd.write(chunk)\n","        return filename.__str__()\n","    except Exception as e:\n","        print(e)\n","        print('Error downloading: ', url, res)\n","        return ''\n","\n","df_documents['document_path'] = df_documents.apply(lambda x: download_doc(x['url']),axis=1)"]},{"cell_type":"markdown","metadata":{"id":"v4iXPw_qkexE"},"source":["We save the file as csv and add any other enteries along with actual files to the folder for the next part. Also the pdf files are downloaded to folder 'downloaded_documents' in root. Move the same to step2 input folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xS9k30R7klLe"},"outputs":[],"source":["path_to_step2 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/output/\"\n","# df_documents.to_csv(path_to_step2+'document_list_updated.csv')"]},{"cell_type":"markdown","metadata":{"id":"bYqooqx0pfF4"},"source":["## Create Paragraphs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"huHk6OoSpn2t"},"outputs":[],"source":["path_to_downloaded_docs = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/input/\"\n","df_documents= pd.read_csv(path_to_step2 +'document_list_updated.csv')\n","\n","import utils\n","from utils.preprocessing import UdfPreProcessor, FileConverter\n","from haystack.schema import Document\n","def extract_full_text(doc_path, file_name=None):\n","    \"\"\"\n","    extracts the raw text from pdf/docx file using haystack utils\n","\n","    Params\n","    --------------\n","    doc_path:  file path which needs text data extraction\n","    file_name: Filename\n","\n","\n","    Return\n","    ------------\n","    output: raw text from the file\n","    \"\"\"\n","    # will use Fileconvertor to extract text\n","    if doc_path != '' and doc_path != 'nan' and doc_path != '..\\\\nan':\n","        convertor = FileConverter()\n","        doc_path = doc_path.replace(\"../data/\",path_to_downloaded_docs)\n","        try:\n","            output = convertor.run(file_path = doc_path, file_name = doc_path.split(\"/\")[-1])\n","        except Exception as e:\n","            print(e)\n","            output = None\n","        if output:\n","            output = output[0]['documents'][0]\n","            # output = output.content\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7rdxoU-tuIx"},"outputs":[],"source":["# harmonizing data with already created climate Watch html paragraph data\n","df_documents['file_name'] = df_documents.url.apply(lambda x: str(x).split('/')[-1].replace('%','_'))\n","df_documents.drop(columns = ['Unnamed: 0', 'backend_status', 'current', 'date','url',\n","                             'version_number', 'comment','annex_type', 'income_group',\n","                             'region', 'eu27', 'ghg_total','ghg_transport','mena'], inplace=True)\n","df_documents.rename(columns = {'document_path':'filepath'}, inplace=True)\n","\n","\n","# extract text from file\n","df_documents['original_text'] = None\n","for i in tqdm(range(len(df_documents))):\n","    df_documents.loc[i,'original_text'] = extract_full_text(df_documents.loc[i]['filepath'])\n","\n","# detect the langauge of text\n","import langdetect\n","df_documents['original_text'] = df_documents.original_text.apply(lambda x: x.content if x else None)\n","df_documents['language'] = df_documents.original_text.apply(lambda x: langdetect.detect(x) if x else None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlndLyEC2Iip"},"outputs":[],"source":["# detect the langauge of text\n","import langdetect\n","df_documents['original_text'] = df_documents.original_text.apply(lambda x: x.content if x else None)\n","df_documents['language'] = df_documents.original_text.apply(lambda x: langdetect.detect(x) if x else None)\n","df_documents.language.unique()"]},{"cell_type":"markdown","metadata":{"id":"fBq0dRaRyeHw"},"source":["Verify the langauge if you see some weird results and update them. Langdetect is not deterministic, there can be sometimes variations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUzfRll6wWXs"},"outputs":[],"source":["# updating some entreies\n","# df_documents.loc[68,'language'] = 'en'\n","df_documents.dropna(subset = ['original_text'],inplace = True)\n","df_documents = df_documents.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WmCanCkyvwj"},"outputs":[],"source":["# We can now create paragraphs, the language detection is done earlier to\n","# insure that we can use correct Punkt tokenizer for sentence splitting and words\n","# boundaries\n","\n","df_documents['paragraphs'] = None\n","placeholder = []\n","for i in tqdm(range(len(df_documents))):\n","    try:\n","        placeholder.append(create_paragraphs_text(df_documents.loc[i,'original_text'],\n","                                    get_country_split_strategy(responsestats,df_documents.loc[i,'country_code']),\n","                                    df_documents.loc[i,'language'], df_documents.loc[i,'file_name']))\n","    except Exception as e:\n","        print('index:',i, '\\n',e)\n","        placeholder.append({})\n","df_documents['paragraphs'] = placeholder\n","df_documents.info()\n","df_documents['paragraph_check'] = df_documents.paragraphs.apply(lambda x: True if x else False)"]},{"cell_type":"code","source":["# do the paragraph check for this we use the default value of 180 words for each\n","# split type. We do this so that the while training of Models, we dont get error\n","# due to tensor size mismatch. alternatively you cna also set the truncation =True\n","# for model tokenizers.\n","# df_documents = df_documents[df_documents.paragraph_check == True].reset_index(drop=True)\n","df_documents['paragraphs'] = df_documents.paragraphs.progress_apply(lambda x: paraLengthCheck(x, keep_hash=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hUqT4AaDxjl","executionInfo":{"status":"ok","timestamp":1702974676588,"user_tz":-60,"elapsed":3617,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"9ba3eea7-8382-44bf-9773-67a93951167f"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 370/370 [00:03<00:00, 109.72it/s]\n"]}]},{"cell_type":"code","source":["df_documents.drop(columns  = ['country','paragraphs1', 'paragraph_check'], inplace=True)\n","df_documents.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BlGSAamlMubx","executionInfo":{"status":"ok","timestamp":1702974919697,"user_tz":-60,"elapsed":331,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"c02a8a1c-4b20-413f-f1d6-7d8b31fd99eb"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 370 entries, 0 to 369\n","Data columns (total 7 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   country_code      370 non-null    object\n"," 1   type_of_document  370 non-null    object\n"," 2   filepath          370 non-null    object\n"," 3   file_name         370 non-null    object\n"," 4   original_text     370 non-null    object\n"," 5   language          370 non-null    object\n"," 6   paragraphs        370 non-null    object\n","dtypes: object(7)\n","memory usage: 20.4+ KB\n"]}]},{"cell_type":"code","source":["# if all is okay then we go ahead and save the dataframe as json\n","import json\n","path_to_step2 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/\"\n","jsonfile = df_documents.to_json(orient=\"records\")\n","parsed = json.loads(jsonfile)\n","with open(path_to_step2 +'output/pdf_para.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)"],"metadata":{"id":"1y_Eb5U0Bh6L","executionInfo":{"status":"ok","timestamp":1702974952883,"user_tz":-60,"elapsed":4619,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["# Harmonize Both Dataset"],"metadata":{"id":"wQiEWvwGQoR1"}},{"cell_type":"code","source":["cw_ndc.type_of_document.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k2OoTU-pQ0V-","executionInfo":{"status":"ok","timestamp":1702976904654,"user_tz":-60,"elapsed":251,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"9c607b43-7cf7-447f-f151-6346fd627748"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['First NDC', 'INDC', 'Revised First NDC', 'Second NDC',\n","       'Archived Second NDC', 'Archived Revised First NDC',\n","       'Revised Second NDC'], dtype=object)"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["df_documents.type_of_document.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvCRpSYLUc1q","executionInfo":{"status":"ok","timestamp":1702976907385,"user_tz":-60,"elapsed":245,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"0f7e380d-016c-484e-8cea-6e3715057243"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['First NDC', 'LTS', 'Revised First NDC', 'Second NDC',\n","       'Archived LTS', 'Archived Revised First NDC',\n","       'NDC reference document'], dtype=object)"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["# updating the file type\n","cw_ndc.type_of_document.replace({'Revised First NDC (archived)':'Archived Revised First NDC'}, inplace=True)"],"metadata":{"id":"AGHdwC7NVM-h","executionInfo":{"status":"ok","timestamp":1702976902046,"user_tz":-60,"elapsed":255,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["import json\n","path_to_step2 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step2/\"\n","jsonfile = df_documents.to_json(orient=\"records\")\n","parsed = json.loads(jsonfile)\n","with open(path_to_step2 +'output/pdf_para.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)\n","\n","\n","jsonfile = cw_ndc.to_json(orient=\"records\")\n","parsed = json.loads(jsonfile)\n","with open(path_to_step2 +'output/html_para.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)"],"metadata":{"id":"9GDvicH3VbCA","executionInfo":{"status":"ok","timestamp":1702976985784,"user_tz":-60,"elapsed":13568,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}}},"execution_count":86,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["kbmNfcxTbz8V","wQiEWvwGQoR1"],"toc_visible":true,"mount_file_id":"1lI2dzx3drg0gbTpslcYw6oArY4aoEnwh","authorship_tag":"ABX9TyNWMEnX5EV7qL6BB2Q8hZwR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}