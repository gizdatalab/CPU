{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"12BLMuWb4kCpcDKfh0jFqFi1K1kSC9ok5","authorship_tag":"ABX9TyNm0EuLvW18mQDwesq1jqTP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Packages Installation(for colab)"],"metadata":{"id":"Jw9vXTuJtQaM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvVw4k86rMPd","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1702911188302,"user_tz":-60,"elapsed":115754,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"f07a5180-93f6-4057-c36f-4a67014a1358"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pycountry\n","  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pycountry\n","Successfully installed pycountry-23.12.11\n","Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n","Installing collected packages: rank_bm25\n","Successfully installed rank_bm25-0.2.2\n","Collecting xlsxwriter\n","  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.1.9\n","Obtaining utils from git+https://github.com/gizdatalab/haystack_utils.git@main#egg=utils\n","  Cloning https://github.com/gizdatalab/haystack_utils.git (to revision main) to ./src/utils\n","  Running command git clone --filter=blob:none --quiet https://github.com/gizdatalab/haystack_utils.git /content/src/utils\n","  Resolved https://github.com/gizdatalab/haystack_utils.git to commit 087e5ae68b5eecbd4ca2415f522a0e8dfc61e617\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting farm-haystack==1.16.0 (from utils)\n","  Downloading farm_haystack-1.16.0-py3-none-any.whl (713 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.1/713.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy==3.4.1 (from utils)\n","  Downloading spacy-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.5.1 (from utils)\n","  Downloading matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nltk==3.7 (from utils)\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pdfplumber==0.6.2 (from utils)\n","  Downloading pdfplumber-0.6.2-py3-none-any.whl (36 kB)\n","Collecting Pillow==9.1.1 (from utils)\n","  Downloading Pillow-9.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seaborn==0.11.2 (from utils)\n","  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.25.1 (from utils)\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting summa==1.2.0 (from utils)\n","  Downloading summa-1.2.0.tar.gz (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting python-docx==0.8.11 (from utils)\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting azure-ai-formrecognizer>=3.2.0b2 (from farm-haystack==1.16.0->utils)\n","  Downloading azure_ai_formrecognizer-3.3.2-py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.1/300.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting boilerpy3 (from farm-haystack==1.16.0->utils)\n","  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n","Collecting canals (from farm-haystack==1.16.0->utils)\n","  Downloading canals-0.11.0-py3-none-any.whl (34 kB)\n","Collecting dill (from farm-haystack==1.16.0->utils)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting events (from farm-haystack==1.16.0->utils)\n","  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (0.19.4)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (4.19.2)\n","Collecting mmh3 (from farm-haystack==1.16.0->utils)\n","  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (10.1.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (3.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (1.5.3)\n","Collecting posthog (from farm-haystack==1.16.0->utils)\n","  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n","Collecting protobuf<=3.20.2 (from farm-haystack==1.16.0->utils)\n","  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (1.10.13)\n","Collecting quantulum3 (from farm-haystack==1.16.0->utils)\n","  Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (0.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (2.31.0)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (1.2.2)\n","Collecting sentence-transformers>=2.2.0 (from farm-haystack==1.16.0->utils)\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sseclient-py (from farm-haystack==1.16.0->utils)\n","  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (8.2.3)\n","Collecting tiktoken>=0.3.0 (from farm-haystack==1.16.0->utils)\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack==1.16.0->utils) (4.66.1)\n","Collecting pdf2image>1.14 (from farm-haystack==1.16.0->utils)\n","  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n","Collecting pytesseract>0.3.7 (from farm-haystack==1.16.0->utils)\n","  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n","Collecting pymupdf>=1.18.16 (from farm-haystack==1.16.0->utils)\n","  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (1.4.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (23.2)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.1->utils) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->utils) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->utils) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->utils) (2023.6.3)\n","Collecting pdfminer.six==20220319 (from pdfplumber==0.6.2->utils)\n","  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Wand>=0.6.7 (from pdfplumber==0.6.2->utils)\n","  Downloading Wand-0.6.13-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx==0.8.11->utils) (4.9.3)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from seaborn==0.11.2->utils) (1.11.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (8.1.12)\n","Collecting wasabi<1.1.0,>=0.9.1 (from spacy==3.4.1->utils)\n","  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (2.0.10)\n","Collecting typer<0.5.0,>=0.3.0 (from spacy==3.4.1->utils)\n","  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (0.10.3)\n","Collecting pydantic (from farm-haystack==1.16.0->utils)\n","  Downloading pydantic-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.4.1->utils) (3.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1->utils) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1->utils) (6.0.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.1->utils)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20220319->pdfplumber==0.6.2->utils) (41.0.7)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20220319->pdfplumber==0.6.2->utils) (5.2.0)\n","Requirement already satisfied: torch!=1.12.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1->utils) (2.1.0+cu121)\n","Collecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils)\n","  Downloading azure_core-1.29.6-py3-none-any.whl (192 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils)\n","  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils)\n","  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (4.5.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack==1.16.0->utils) (2023.6.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack==1.16.0->utils) (2023.3.post1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy==3.4.1->utils) (6.4.0)\n","Collecting PyMuPDFb==1.23.7 (from pymupdf>=1.18.16->farm-haystack==1.16.0->utils)\n","  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->utils) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack==1.16.0->utils) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack==1.16.0->utils) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack==1.16.0->utils) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack==1.16.0->utils) (2023.11.17)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->farm-haystack==1.16.0->utils) (3.2.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->farm-haystack==1.16.0->utils) (0.16.0+cu121)\n","Collecting sentencepiece (from sentence-transformers>=2.2.0->farm-haystack==1.16.0->utils)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->utils) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->utils) (0.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.4.1->utils) (2.1.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack==1.16.0->utils) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack==1.16.0->utils) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack==1.16.0->utils) (0.32.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack==1.16.0->utils) (0.13.2)\n","Collecting monotonic>=1.5 (from posthog->farm-haystack==1.16.0->utils)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting backoff>=1.10.0 (from posthog->farm-haystack==1.16.0->utils)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack==1.16.0->utils) (7.0.0)\n","Collecting num2words (from quantulum3->farm-haystack==1.16.0->utils)\n","  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (3.7.1)\n","Collecting typing-extensions>=4.0.1 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Collecting isodate>=0.6.0 (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils)\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (1.3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7->transformers==4.25.1->utils) (1.12)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7->transformers==4.25.1->utils) (2.1.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->pdfminer.six==20220319->pdfplumber==0.6.2->utils) (1.16.0)\n","Collecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack==1.16.0->utils)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (1.2.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20220319->pdfplumber==0.6.2->utils) (2.21)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack==1.16.0->utils) (3.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7->transformers==4.25.1->utils) (1.3.0)\n","Building wheels for collected packages: python-docx, summa, sentence-transformers, docopt\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184488 sha256=72863c2dabdfa664fdc9155d46b7522ffbbd6c033287cfeb97893fd9bb06e265\n","  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n","  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54388 sha256=271028cac4194442a2be6da32845de6d98b461128380ff2c91eb9aa72bb504ed\n","  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=14dcc366b517bb7173b6244e7cc42ee47225da11ae173fe1beabc09260f80b43\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=cb54a26f67118b84439415b083141d061bd6c047a1c8c69097daad69eb00998f\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built python-docx summa sentence-transformers docopt\n","Installing collected packages: wasabi, Wand, tokenizers, sseclient-py, sentencepiece, monotonic, mmh3, events, docopt, azure-common, typing-extensions, typer, python-docx, PyMuPDFb, protobuf, Pillow, num2words, nltk, isodate, dill, boilerpy3, backoff, tiktoken, summa, pytesseract, pymupdf, pydantic, posthog, pdf2image, matplotlib, canals, azure-core, transformers, seaborn, pdfminer.six, msrest, sentence-transformers, quantulum3, pdfplumber, azure-ai-formrecognizer, spacy, farm-haystack, utils\n","  Attempting uninstall: wasabi\n","    Found existing installation: wasabi 1.1.2\n","    Uninstalling wasabi-1.1.2:\n","      Successfully uninstalled wasabi-1.1.2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.0\n","    Uninstalling tokenizers-0.15.0:\n","      Successfully uninstalled tokenizers-0.15.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.0\n","    Uninstalling typer-0.9.0:\n","      Successfully uninstalled typer-0.9.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.8.1\n","    Uninstalling nltk-3.8.1:\n","      Successfully uninstalled nltk-3.8.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.13\n","    Uninstalling pydantic-1.10.13:\n","      Successfully uninstalled pydantic-1.10.13\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","  Attempting uninstall: seaborn\n","    Found existing installation: seaborn 0.12.2\n","    Uninstalling seaborn-0.12.2:\n","      Successfully uninstalled seaborn-0.12.2\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.6.1\n","    Uninstalling spacy-3.6.1:\n","      Successfully uninstalled spacy-3.6.1\n","  Running setup.py develop for utils\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.4.1 which is incompatible.\n","plotnine 0.12.4 requires matplotlib>=3.6.0, but you have matplotlib 3.5.1 which is incompatible.\n","tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-9.1.1 PyMuPDFb-1.23.7 Wand-0.6.13 azure-ai-formrecognizer-3.3.2 azure-common-1.1.28 azure-core-1.29.6 backoff-2.2.1 boilerpy3-1.0.7 canals-0.11.0 dill-0.3.7 docopt-0.6.2 events-0.5 farm-haystack-1.16.0 isodate-0.6.1 matplotlib-3.5.1 mmh3-4.0.1 monotonic-1.6 msrest-0.7.1 nltk-3.7 num2words-0.5.13 pdf2image-1.16.3 pdfminer.six-20220319 pdfplumber-0.6.2 posthog-3.1.0 protobuf-3.20.2 pydantic-1.9.2 pymupdf-1.23.7 pytesseract-0.3.10 python-docx-0.8.11 quantulum3-0.9.0 seaborn-0.11.2 sentence-transformers-2.2.2 sentencepiece-0.1.99 spacy-3.4.1 sseclient-py-1.8.0 summa-1.2.0 tiktoken-0.5.2 tokenizers-0.13.3 transformers-4.25.1 typer-0.4.2 typing-extensions-4.9.0 utils-1.0.1 wasabi-0.10.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","google","matplotlib","mpl_toolkits"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=138974fc8d73ea928a87ca74245e9b80d5181075340e56a913d94c31cf6073b1\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["!pip3 install pycountry"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from typing import Callable, Dict, List, Optional, Text, Tuple, Union\n","import pycountry\n","from bs4 import BeautifulSoup\n","from IPython.display import display\n","import os"],"metadata":{"id":"GKHw8k67vXwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Collect Responses"],"metadata":{"id":"rhMn3Pz5uxAe"}},{"cell_type":"markdown","source":["## Processing Functions"],"metadata":{"id":"J-k7d8JjvPoe"}},{"cell_type":"code","source":["def getCWResponse(path_to_cwdata:str)-> pd.DataFrame:\n","    \"\"\"\n","    This function relies upon the \"NDC Content\" subsection of data from\n","    https://www.climatewatchdata.org/data-explorer/\n","    with 3 important files:\n","      1. CW_NDC_data_sector\n","      2. CW_NDC_metadata\n","      3. CW_NDC_tracker\n","\n","    Params\n","    -----------\n","    path_to_cwdata: path to the NDC Content data from Climate Watch\n","\n","\n","    Return\n","    --------\n","    cw_data: Dataframe\n","\n","    \"\"\"\n","    # sector data file has the Climate watch response data\n","    data_sector  = pd.read_csv(path_to_cwdata+'CW_NDC_data_sector.csv')\n","    # we use the metadata to incorporate more info to response data\n","    ndc_metadata = pd.read_csv(path_to_cwdata+'CW_NDC_metadata.csv')\n","    tracker  = pd.read_csv(path_to_cwdata+'CW_NDC_tracker.csv')\n","\n","    # Drop NA for country and question code\n","    cw_data = data_sector[data_sector.QuestionCode.notna()]\n","    cw_data = cw_data[cw_data.Country.notna()]\n","    cw_data = cw_data.reset_index(drop = True)\n","    cw_data['Sector'] = cw_data.Sector.str.strip()\n","    cw_data['SubSector'] = cw_data.SubSector.str.strip()\n","    cw_data['QuestionCode'] = cw_data.QuestionCode.str.strip()\n","    cw_data['ResponseText'] = cw_data.ResponseText.str.strip()\n","    # Getting metadata information\n","    cols_metadata = ndc_metadata[ndc_metadata.column_name != 'ghg_target_type'].set_index(\n","                                                    'column_name').T.to_dict('list')\n","\n","    # Appending all info from metadata to the sector data dataframe\n","    cw_data['GlobalCategory'] =  cw_data['QuestionCode'].apply(\n","                                        lambda x: cols_metadata[x][0] if x in\n","                                        cols_metadata.keys() else None)\n","    cw_data['OverviewCategory'] =  cw_data['QuestionCode'].apply(\n","                                          lambda x: cols_metadata[x][1] if x in\n","                                          cols_metadata.keys() else None)\n","\n","    cw_data['QuestionText'] =  cw_data['QuestionCode'].apply(lambda x:\n","                                    cols_metadata[x][3] if x in cols_metadata.keys()\n","                                      else None)\n","    cw_data['QuestionDefinition'] =  cw_data['QuestionCode'].apply(\n","                                              lambda x: cols_metadata[x][4] if x in\n","                                              cols_metadata.keys() else None)\n","    cw_data['GroupIndicator'] =  cw_data['QuestionCode'].apply(\n","                                        lambda x: cols_metadata[x][5] if x in\n","                                        cols_metadata.keys() else None)\n","    cw_data['Source'] =  cw_data['QuestionCode'].apply(lambda x:\n","                            cols_metadata[x][6] if x in cols_metadata.keys() else None)\n","\n","    # Sector data has country specified by ISO Alpha2 while NDC are\n","    # listed as per ISO Alpha3, rectifying the same\n","\n","    cw_data = cw_data.rename(columns = {'Country':'Alpha2'})\n","\n","    countryList = pd.DataFrame(columns = ['alpha2','alpha3','name','numeric'])\n","    for i in pycountry.countries:\n","        countryList.loc[len(countryList)] = [i.alpha_2, i.alpha_3, i.name, i.numeric]\n","    alpha2_alpha3 = dict(zip(countryList.alpha2, countryList.alpha3))\n","\n","    # using the country name from climatewatch data for homogeneity\n","    alpha3_country = dict(zip(tracker.ISO, tracker.Country))\n","\n","    # adding ISO Alpha3 and Country names to dataframe\n","    cw_data['Alpha3'] = cw_data['Alpha2'].apply(lambda x:\n","                            alpha2_alpha3[x] if x in alpha2_alpha3.keys() else None)\n","    cw_data['Country'] = cw_data['Alpha3'].apply(lambda x:\n","                          alpha3_country[x] if x in alpha3_country.keys() else None)\n","    print(len(cw_data))\n","\n","    # Cleaning for HTML text\n","    html_df = cw_data[cw_data.ResponseText.str.contains(\"<p>\", na = False)]\n","    html_df = html_df.reset_index(drop=True)\n","    print(\"HTML text counts\",len(html_df))\n","    html_df['ResponseText'] = html_df['ResponseText'].apply(lambda x:\n","                                          BeautifulSoup(x).get_text(separator = '|'))\n","\n","    cw_data = cw_data[~cw_data.ResponseText.str.contains(\"<p>\",na = False)]\n","\n","    print(\"Without HTML\",len(cw_data))\n","    cw_data = pd.concat([cw_data,html_df], ignore_index = True)\n","    print(\"Dataframe length\",len(cw_data))\n","\n","    # Response Text has Separator ('|', ';'), which signifies there are mutltiple response\n","    # text in same row. Need to split this across different rows and duplicating all\n","    # other columns info\n","\n","    cw_data[\"ResponseText\"]=cw_data[\"ResponseText\"].str.split(\"|\")\n","    cw_data = cw_data.explode(\"ResponseText\").reset_index(drop=True)\n","\n","    cw_data[\"ResponseText\"]=cw_data[\"ResponseText\"].str.split(\"<br>\")\n","    cw_data = cw_data.explode(\"ResponseText\").reset_index(drop=True)\n","\n","    cw_data[\"ResponseText\"]=cw_data[\"ResponseText\"].str.split(\";\")\n","    cw_data = cw_data.explode(\"ResponseText\").reset_index(drop=True)\n","\n","    cw_data['ResponseText'] = cw_data.ResponseText.str.strip()\n","    cw_data = cw_data.dropna(subset = ['Alpha3'])\n","\n","    print(\"Dataframe length after explode\", len(cw_data))\n","    cw_data.drop(['GlobalCategory','GroupIndicator'], axis=1, inplace=True)\n","\n","    cw_data = cw_data.drop_duplicates()\n","    cw_data = cw_data.reset_index(drop = True)\n","    cw_data['Source'] = 'CW'\n","    print(\"df length without duplicates\", len(cw_data))\n","\n","    cw_data['CWInfo'] = cw_data[['Alpha2','Sector', 'SubSector', 'QuestionCode',\n","          'OverviewCategory', 'QuestionText','QuestionDefinition']].to_dict('records')\n","\n","    cw_data.drop(['Alpha2','Sector', 'SubSector', 'QuestionCode',\n","          'OverviewCategory', 'QuestionText','QuestionDefinition'],axis=1, inplace = True)\n","\n","    cw_data['Document'] = cw_data.Document.str.strip()\n","\n","    return cw_data\n","\n","\n","def getIKIResposnse(path_to_iki:str) -> Tuple[pd.DataFrame,pd.DataFrame]:\n","    \"\"\"\n","    This function relies upon the \"IKI Data\" which has 2 important files:\n","      1. 20230125_NDC-Database-Analysis.xlsx\n","      2. code_book.xlsx\n","\n","\n","    Params\n","    -----------\n","    path_to_iki: path to the IKI data\n","\n","\n","    Return\n","    --------\n","    df_concat: Dataframe with IKI responses\n","    df_documents: Dataframe with list of documents relied upon to create IKI\n","                  responses\n","\n","    \"\"\"\n","    # Pull IKI data one tab at a time\n","    file_name = '20230125_NDC-Database-Analysis.xlsx'\n","\n","    # read each tab and store as separate object\n","    df_netzero = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                              sheet_name=\"Net-zero\", index_col=None,\n","                              na_values=['NA'], usecols='A:Q', skiprows=7)\n","    df_targets = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                              sheet_name=\"Targets\", index_col=None,\n","                              na_values=['NA'], usecols='A:Y', skiprows=7)\n","    df_mitigation = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                  sheet_name=\"Mitigation\", index_col=None,\n","                                  na_values=['NA'], usecols='A:AP', skiprows=8)\n","    df_adaptation = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                  sheet_name=\"Adaptation\", index_col=None,\n","                                  na_values=['NA'], usecols='A:AN', skiprows=7)\n","    df_governance = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                  sheet_name=\"Governance\", index_col=None,\n","                                  na_values=['NA'], usecols='A:I', skiprows=7)\n","    df_implementation = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                      sheet_name=\"Implementation\", index_col=None,\n","                                      na_values=['NA'], usecols='A:I', skiprows=7)\n","    df_investment = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                  sheet_name=\"Investment\", index_col=None,\n","                                  na_values=['NA'], usecols='A:I', skiprows=7)\n","    df_benefits = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                sheet_name=\"Benefits\", index_col=None,\n","                                na_values=['NA'], usecols='A:I', skiprows=7)\n","    df_covid = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                            sheet_name=\"COVID19\", index_col=None, na_values=['NA'],\n","                            usecols='A:I', skiprows=7)\n","    df_documents = pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                                sheet_name=\"Document\", index_col=None,\n","                                na_values=['NA'], usecols='A:P', skiprows=7)\n","    # Make the variable names machine-friendly\n","    file_name = 'code_book.xlsx'\n","    df_netzero.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"netzero\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_targets.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"targets\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_mitigation.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"mitigation\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_adaptation.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"adaptation\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_governance.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"governance\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_implementation.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name),\n","                      sheet_name=\"implementation\", index_col=None, usecols='B')['var_name']\n","    df_investment.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"investment\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_benefits.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"benefits\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_covid.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"covid19\",\n","                      index_col=None, usecols='B')['var_name']\n","    df_documents.columns = \\\n","        pd.read_excel(os.path.join('..', path_to_iki, file_name), sheet_name=\"documents\",\n","                      index_col=None, usecols='B')['var_name']\n","    # adding tab name to keep track\n","    df_netzero['IkiSheet'] = 'netzero'\n","    df_targets['IkiSheet'] = 'targets'\n","    df_mitigation['IkiSheet'] = 'mitigation'\n","    df_adaptation['IkiSheet'] = 'adaptation'\n","    df_governance['IkiSheet'] = 'governance'\n","    df_implementation['IkiSheet'] = 'implementation'\n","    df_investment['IkiSheet'] = 'investment'\n","    df_benefits['IkiSheet'] = 'benefits'\n","    df_covid['IkiSheet'] = 'covid19'\n","\n","    # merge all sources into one dataframe for simplicity\n","    # we do this by normalizing the dataframe\n","    keep_cols = {'country_code', 'country', 'type_of_document',\n","                'content'}\n","\n","    df_netzero = normalizedf(df_netzero,keep_cols,'IkiInfo')\n","    df_targets = normalizedf(df_targets, keep_cols, 'IkiInfo')\n","    df_mitigation = normalizedf(df_mitigation, keep_cols, 'IkiInfo')\n","    df_adaptation = normalizedf(df_adaptation, keep_cols, 'IkiInfo')\n","    df_governance = normalizedf(df_governance, keep_cols, 'IkiInfo')\n","    df_implementation = normalizedf(df_implementation, keep_cols, 'IkiInfo')\n","    df_investment = normalizedf(df_investment, keep_cols, 'IkiInfo')\n","    df_benefits = normalizedf(df_benefits, keep_cols, 'IkiInfo')\n","    df_covid = normalizedf(df_covid, keep_cols, 'IkiInfo')\n","\n","    df_concat = pd.concat([df_netzero, df_targets,df_mitigation, df_adaptation,\n","                       df_governance,df_implementation,df_investment, df_benefits,\n","                       df_covid], ignore_index =True)\n","\n","    # df_concat = df_concat.astype(str)\n","    df_concat['Source'] = 'IKITracs'\n","    print(f\"Number of imported refs: {len(df_concat)}\")\n","\n","    df_concat.rename(columns = {'country_code':'Alpha3',\n","                            'type_of_document':'Document','country':'Country',\n","                            'content':'ResponseText'}, inplace = True)\n","\n","    df_concat = df_concat.reset_index(drop=True)\n","    df_concat.ResponseText = df_concat.ResponseText.str.strip()\n","    df_concat.Document = df_concat.Document.str.strip()\n","\n","    return df_concat, df_documents\n","\n","def normalizedf(df:pd.DataFrame,keep_cols:set,col_name:str):\n","    \"\"\"\n","    Takes a dataframe and Normalizes it, 'keep_cols' are kept intact and other\n","    are fed to dictionary object 'col_name' which becomes attribute of each row.\n","\n","    Params\n","    --------------\n","    df: Dataframe which needs to be normalized\n","    keep_cols: Set of names of columns which need to be kept as it is, while\n","              other columns are converted into dictionary and saved in under one\n","              column which derives its name from param 'col_name'.\n","    col_name: dictionary object created using remaining columns except 'keep_cols'\n","              will be saved under the column col_name\n","\n","    Return\n","    ---------------\n","    df: Normalized Dataframe\n","\n","    \"\"\"\n","    remove_cols = list(set(df.columns) - keep_cols)\n","    df[col_name] = df[remove_cols].to_dict('records')\n","    df.drop(remove_cols,axis=1,inplace= True)\n","    return df"],"metadata":{"id":"qK9ojW_KuxdK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def basicReponseProcessing(data):\n","    \"\"\"\n","        takes dataframe and drops the responses which are very generic or of\n","        not much use\n","    \"\"\"\n","    data = data[~data['ResponseText'].str.contains(\"http\",na=False)]\n","    # we drop some of the most common occuring responses which are generic and\n","    # dont reflect true response\n","    data= data[~((data.ResponseText == 'Not Available')|\n","                    (data.ResponseText == 'Other')|(data.ResponseText == 'Water')|\n","                    (data.ResponseText == 'Food and Nutrition Security')|\n","                    (data.ResponseText == 'Disaster Risk Management'))]\n","    data = data[data.ResponseWordcount > 0]\n","    data = data.reset_index(drop = True)\n","    print(\"df after some basic cleaning/processing:\",len(data))\n","\n","    return data\n","\n","def percentile(n):\n","    \"\"\"\n","    helping function to get the percentile value for aggregate/groupby\n","    https://stackoverflow.com/questions/17578115/pass-percentiles-to-pandas-agg-function\n","\n","    \"\"\"\n","    def percentile_(x):\n","        return np.percentile(x, n, method = 'median_unbiased')\n","    percentile_.__name__ = 'percentile_%s' % n\n","    return percentile_\n","\n","def getCountryReponsestatistics(df):\n","    \"\"\"\n","    gets the statistics of Responseword count for each country\n","\n","    Params\n","    ------------\n","    df: dataframe which contains the responses from both Climate Watch and IKI\n","        This require to have Country Code, ResponseText and ResponseWordcount columns\n","\n","\n","    Return\n","    ----------------\n","    country_: dataframe which country response statistics\n","    responseLengthMean:global mean = mean of country_mean\n","    responseLengthMedian: global median != mean of country_median\n","    responseLengthMax:  ninetyfifthpercentile of country_ninetyfifthpercentile\n","\n","    \"\"\"\n","    country_ = df.groupby(['Alpha3','Country'], as_index=False).agg(\n","          value_count=('ResponseText','size'),mean_val=('ResponseWordcount',np.mean),\n","          median_val=('ResponseWordcount',np.median),min_val=('ResponseWordcount',min),\n","          max_val=('ResponseWordcount',max),std_val=('ResponseWordcount',np.std),\n","          ninetyfifth_percentile=('ResponseWordcount',percentile(95)))\n","\n","    responseLengthMean = np.mean(country_.mean_val)\n","    responseLengthMedian = np.median(df.ResponseWordcount)\n","\n","    # getting the 95 percentile value, As Max value doesnt\n","    # reflect good assessment\n","    responseLengthMax = np.percentile(country_.ninetyfifth_percentile, 95)\n","\n","    # getting the aggregates at Global level.\n","    country_.iloc[-1] = ['Global','Global',None, responseLengthMean,\n","                      responseLengthMedian, None, responseLengthMax, None, None]\n","    country_.index = country_.index +1\n","\n","    return country_ , responseLengthMean, responseLengthMedian, responseLengthMax"],"metadata":{"id":"NRwJAkz0ytJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main Code"],"metadata":{"id":"UtOhytHV0Hud"}},{"cell_type":"code","source":["# setting up the paths to climate watch data\n","path_to_cwdata = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step1/input/ClimateWatch/\"\n","cw_responses = getCWResponse(path_to_cwdata)\n","\n","# set path to iki tracs data\n","path_to_iki = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step1/input/IKI/\"\n","iki_responses, iki_document_list = getIKIResposnse(path_to_iki)\n","\n","# combine both response datasets from Climat Watch and IKI\n","cw_iki = pd.concat([cw_responses,iki_responses], axis=0, ignore_index= True)\n","cw_iki = cw_iki.reset_index(drop = True)\n","print(\"length of df:\", len(cw_iki))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgDLwfzpeNhH","executionInfo":{"status":"ok","timestamp":1702911495675,"user_tz":-60,"elapsed":16207,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"714879d5-5cf1-4834-8bec-cc68161f51fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["80867\n","HTML text counts 1459\n","Without HTML 79408\n","Dataframe length 80867\n","Dataframe length after explode 91444\n","df length without duplicates 66075\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n","  warn(msg)\n","/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n","  warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Number of imported refs: 4226\n","length of df: 70301\n"]}]},{"cell_type":"code","source":["# getting word counts in Response for each row\n","cw_iki['ResponseWordcount'] = cw_iki.ResponseText.str.split().str.len()\n","\n","# dropping some not useful responses\n","cw_iki = basicReponseProcessing(cw_iki)\n","\n","# get some response word count statistics\n","# responseLengthMean = global mean = mean of country_mean\n","# responseLengthMedian = global median != mean of country_median\n","# responseLengthMax = ninetyfifthpercentile of country_ninetyfifthpercentile\n","responsestats,responseLengthMean, responseLengthMedian,responseLengthMax  = \\\n","                                            getCountryReponsestatistics(cw_iki)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgdWZ3T6zXBt","executionInfo":{"status":"ok","timestamp":1702911502237,"user_tz":-60,"elapsed":731,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"b11a7822-ecc3-4037-cb75-b6e8eb8750f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["df after some basic cleaning/processing: 51060\n"]}]},{"cell_type":"code","source":["print(f\"Response length Mean is {responseLengthMean}\")\n","print(f\"Response length Median is {responseLengthMedian}\")\n","print(f\"Response length Max Length is {responseLengthMax}\")\n","display(responsestats.head())\n","display(cw_iki.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"lPQil1KwTIkq","executionInfo":{"status":"ok","timestamp":1702911505672,"user_tz":-60,"elapsed":12,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"9872c461-342d-4b1c-e54c-295befe218bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Response length Mean is 17.06963864364939\n","Response length Median is 10.0\n","Response length Max Length is 85.7875\n"]},{"output_type":"display_data","data":{"text/plain":["  Alpha3               Country  value_count   mean_val  median_val  min_val  \\\n","1    AFG           Afghanistan        308.0   7.974026         7.0      1.0   \n","2    AGO                Angola        473.0   7.452431         5.0      1.0   \n","3    ALB               Albania        467.0  18.171306        14.0      1.0   \n","4    AND               Andorra         56.0  29.571429        25.0      3.0   \n","5    ARE  United Arab Emirates        320.0  32.996875        30.0      2.0   \n","\n","   max_val    std_val  ninetyfifth_percentile  \n","1     51.0   6.793317                    20.5  \n","2     39.0   6.094103                    21.0  \n","3    113.0  14.971382                    46.0  \n","4    364.0  48.657563                    57.5  \n","5    118.0  21.726499                    75.0  "],"text/html":["\n","  <div id=\"df-59800214-6e06-4c1b-bd97-a11dbaa27dbc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Alpha3</th>\n","      <th>Country</th>\n","      <th>value_count</th>\n","      <th>mean_val</th>\n","      <th>median_val</th>\n","      <th>min_val</th>\n","      <th>max_val</th>\n","      <th>std_val</th>\n","      <th>ninetyfifth_percentile</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>AFG</td>\n","      <td>Afghanistan</td>\n","      <td>308.0</td>\n","      <td>7.974026</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>51.0</td>\n","      <td>6.793317</td>\n","      <td>20.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AGO</td>\n","      <td>Angola</td>\n","      <td>473.0</td>\n","      <td>7.452431</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>39.0</td>\n","      <td>6.094103</td>\n","      <td>21.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ALB</td>\n","      <td>Albania</td>\n","      <td>467.0</td>\n","      <td>18.171306</td>\n","      <td>14.0</td>\n","      <td>1.0</td>\n","      <td>113.0</td>\n","      <td>14.971382</td>\n","      <td>46.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AND</td>\n","      <td>Andorra</td>\n","      <td>56.0</td>\n","      <td>29.571429</td>\n","      <td>25.0</td>\n","      <td>3.0</td>\n","      <td>364.0</td>\n","      <td>48.657563</td>\n","      <td>57.5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ARE</td>\n","      <td>United Arab Emirates</td>\n","      <td>320.0</td>\n","      <td>32.996875</td>\n","      <td>30.0</td>\n","      <td>2.0</td>\n","      <td>118.0</td>\n","      <td>21.726499</td>\n","      <td>75.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59800214-6e06-4c1b-bd97-a11dbaa27dbc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-59800214-6e06-4c1b-bd97-a11dbaa27dbc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-59800214-6e06-4c1b-bd97-a11dbaa27dbc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b2924a98-39e8-49d6-aba3-159ca843db0d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2924a98-39e8-49d6-aba3-159ca843db0d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b2924a98-39e8-49d6-aba3-159ca843db0d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 51060 entries, 0 to 51059\n","Data columns (total 8 columns):\n"," #   Column             Non-Null Count  Dtype  \n","---  ------             --------------  -----  \n"," 0   Document           51053 non-null  object \n"," 1   ResponseText       51060 non-null  object \n"," 2   Source             51060 non-null  object \n"," 3   Alpha3             51053 non-null  object \n"," 4   Country            51052 non-null  object \n"," 5   CWInfo             46973 non-null  object \n"," 6   IkiInfo            4087 non-null   object \n"," 7   ResponseWordcount  51060 non-null  float64\n","dtypes: float64(1), object(7)\n","memory usage: 3.1+ MB\n"]},{"output_type":"display_data","data":{"text/plain":["None"]},"metadata":{}}]},{"cell_type":"code","source":["print(f\"Number of rows with Response length > 85 is {sum(cw_iki.ResponseWordcount>85)}\")\n","print(f\"Number of rows with Response length > 60 is {sum(cw_iki.ResponseWordcount>60)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24Z5nYM50GCx","executionInfo":{"status":"ok","timestamp":1702889756485,"user_tz":-60,"elapsed":193,"user":{"displayName":"Prashant Singh","userId":"01066843539489261366"}},"outputId":"7e03c50b-d586-4236-bd7d-4d14b590b5cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows with Response length > 85 is 366\n","Number of rows with Response length > 60 is 1091\n"]}]},{"cell_type":"code","source":["# comment out and run the cell to save the data\n","import json\n","path_to_step1 = \"/content/drive/MyDrive/Colab Notebooks/CPU/Step1/output/\"\n","jsonfile = cw_iki.to_json(orient = 'records')\n","parsed = json.loads(jsonfile)\n","with open(path_to_step1 +'cwiki_responses.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)\n","\n","# save documents list\n","jsonfile = iki_document_list.to_json(orient = 'records')\n","parsed = json.loads(jsonfile)\n","with open(path_to_step1 +'documents_list.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)\n","\n","# save country wide response statistics\n","jsonfile = responsestats.to_json(orient = 'records')\n","parsed = json.loads(jsonfile)\n","with open(path_to_step1 +'responsestats.json', 'w') as file:\n","    json.dump(parsed, file, indent=4)"],"metadata":{"id":"bLEGKCfYXxeO"},"execution_count":null,"outputs":[]}]}